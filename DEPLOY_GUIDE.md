# ðŸŽ¯ HÆ¯á»šNG DáºªN DEPLOY - Cho Developer

## ðŸ“‹ Chuáº©n bá»‹ trÆ°á»›c khi deploy

### 1. Train AI Model OFFLINE (1 láº§n duy nháº¥t)

```powershell
# Cháº¡y script training trong VSCode
D:/bookweb/.venv/Scripts/python.exe train_offline.py
```

**QuÃ¡ trÃ¬nh:**
- â±ï¸ Thá»i gian: 5-8 giá» cho 10,000+ sÃ¡ch
- ðŸ’¾ Auto-save má»—i 50 sÃ¡ch
- â¸ï¸ CÃ³ thá»ƒ dá»«ng (Ctrl+C) vÃ  cháº¡y láº¡i Ä‘á»ƒ tiáº¿p tá»¥c
- âœ… Cháº¡y xong 1 láº§n, khÃ´ng cáº§n train láº¡i

**Káº¿t quáº£:**
```
data/checkpoints/
â”œâ”€â”€ latest_checkpoint.pkl  # Training state
â”œâ”€â”€ embeddings.npy         # AI embeddings
â””â”€â”€ metadata.json          # Book metadata
```

### 2. Test locally

```powershell
.\run.ps1
```

Kiá»ƒm tra:
- âœ… Search hoáº¡t Ä‘á»™ng (khÃ´ng cáº§n train trÃªn web)
- âœ… Voice search hoáº¡t Ä‘á»™ng
- âœ… Text-to-speech hoáº¡t Ä‘á»™ng
- âœ… Äá»c sÃ¡ch + highlight

### 3. Chuáº©n bá»‹ files Ä‘á»ƒ deploy

```
bookweb/
â”œâ”€â”€ backend/              # Deploy nÃ y
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ frontend/             # Build rá»“i deploy
â”‚   â””â”€â”€ dist/            # After npm run build
â”œâ”€â”€ ml_model/            # Deploy nÃ y
â”‚   â”œâ”€â”€ book_embedding.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/                # Deploy nÃ y (QUAN TRá»ŒNG!)
â”‚   â””â”€â”€ checkpoints/     # Trained model
â”‚       â”œâ”€â”€ embeddings.npy    # ~500MB
â”‚       â”œâ”€â”€ metadata.json
â”‚       â””â”€â”€ latest_checkpoint.pkl
```

## ðŸš€ Deploy lÃªn Production

### Option 1: Vercel (Frontend) + Railway/Render (Backend)

#### Frontend (Vercel):
```powershell
cd frontend
npm run build
# Deploy dist/ folder to Vercel
```

#### Backend (Railway/Render):
```bash
# Äáº£m báº£o cÃ³ cÃ¡c file:
backend/
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Procfile (for Heroku/Railway)
data/checkpoints/  # Upload trained model!
ml_model/
```

**Procfile:**
```
web: python backend/app.py
```

### Option 2: Docker (Full Stack)

#### Dockerfile:
```dockerfile
FROM python:3.11

WORKDIR /app

# Copy backend
COPY backend/ ./backend/
COPY ml_model/ ./ml_model/
COPY data/ ./data/

# Install dependencies
RUN pip install -r backend/requirements.txt

# Copy frontend build
COPY frontend/dist/ ./frontend/dist/

EXPOSE 5000

CMD ["python", "backend/app.py"]
```

### Option 3: VPS (Ubuntu/Linux)

```bash
# 1. Upload code
scp -r bookweb/ user@server:/var/www/

# 2. Setup server
ssh user@server
cd /var/www/bookweb

# 3. Install Python deps
python3 -m venv venv
source venv/bin/activate
pip install -r backend/requirements.txt

# 4. Install Node & build frontend
npm install
npm run build

# 5. Setup Nginx
# Point to frontend/dist/ and proxy /api to :5000

# 6. Run with PM2
pm2 start backend/app.py --name bookweb
pm2 startup
pm2 save
```

## âš ï¸ QUAN TRá»ŒNG khi Deploy

### 1. Báº¯t buá»™c pháº£i cÃ³ trained model!

```
âŒ KHÃ”NG deploy náº¿u chÆ°a cÃ³:
data/checkpoints/embeddings.npy
data/checkpoints/metadata.json

âœ… Pháº£i train xong trÆ°á»›c khi deploy
```

### 2. Update backend config

```python
# backend/app.py
DATASET_PATH = os.getenv('DATASET_PATH', '/app/data/books')
CHECKPOINT_DIR = os.getenv('CHECKPOINT_DIR', '/app/data/checkpoints')
```

### 3. Environment variables

```env
DATASET_PATH=/app/data/books
CHECKPOINT_DIR=/app/data/checkpoints
FLASK_ENV=production
PORT=5000
```

### 4. Frontend API URL

```javascript
// frontend/src/api.js
const API_BASE_URL = import.meta.env.VITE_API_URL || 'https://your-backend.com';
```

## ðŸ“Š Checklist trÆ°á»›c khi deploy

- [ ] âœ… ÄÃ£ train AI model offline (train_offline.py)
- [ ] âœ… File embeddings.npy tá»“n táº¡i (~500MB)
- [ ] âœ… File metadata.json tá»“n táº¡i
- [ ] âœ… Test search locally hoáº¡t Ä‘á»™ng
- [ ] âœ… Frontend build thÃ nh cÃ´ng (npm run build)
- [ ] âœ… Backend requirements.txt Ä‘áº§y Ä‘á»§
- [ ] âœ… Environment variables Ä‘Ã£ set
- [ ] âœ… CORS configured Ä‘Ãºng
- [ ] âœ… File size check (embeddings cÃ³ thá»ƒ lá»›n)

## ðŸŽ¯ Workflow Deploy

```
1. Developer (You):
   â””â”€â†’ Run train_offline.py (1 láº§n)
   â””â”€â†’ Commit trained model (data/checkpoints/)
   â””â”€â†’ Push to git/server

2. Production Server:
   â””â”€â†’ Pull code + trained model
   â””â”€â†’ Install dependencies
   â””â”€â†’ Run backend
   â””â”€â†’ Users access web â†’ No training needed! âœ…
```

## ðŸ“ Notes

### Táº¡i sao pháº£i train offline?
- â±ï¸ Training 10,000 sÃ¡ch máº¥t nhiá»u giá»
- ðŸ’° Server costs cao náº¿u train trÃªn production
- ðŸš€ Users muá»‘n dÃ¹ng ngay, khÃ´ng Ä‘á»£i training
- âœ… Train 1 láº§n, deploy nhiá»u láº§n

### Dataset storage
- ðŸ“¦ Dataset gá»‘c: ~541MB (10,000 files .txt)
- ðŸ§  Embeddings: ~500MB (numpy array)
- ðŸ“„ Metadata: ~5MB (JSON)
- **Total: ~1GB cáº§n upload lÃªn server**

### Alternatives náº¿u server storage nhá»:
1. DÃ¹ng S3/Cloud Storage cho dataset
2. Load embeddings tá»« remote
3. Compress embeddings (lossy)

## ðŸ†˜ Troubleshooting

### "No embeddings found"
```bash
# Check files exist
ls data/checkpoints/
# Should see: embeddings.npy, metadata.json

# If missing, run train_offline.py
python train_offline.py
```

### "Memory error loading embeddings"
```python
# Use memory mapping for large files
embeddings = np.load('embeddings.npy', mmap_mode='r')
```

### "Search not working"
```python
# Check model initialized
curl http://localhost:5000/api/training/status
# Should show: embeddings_count > 0
```

---

## âœ… Summary

1. **Train once offline:** `python train_offline.py`
2. **Commit trained model:** `data/checkpoints/`
3. **Deploy everything:** Backend + Frontend + Data
4. **Users enjoy:** No training needed!

ðŸŽ‰ Ready to deploy!
